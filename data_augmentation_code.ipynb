{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fed691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "data_dir = './imbalanced/imbalanced/'  # load data\n",
    "train_dir = os.path.join(data_dir,'train/') # load training data\n",
    "\n",
    "data_transform = transforms.Compose([transforms.Resize([224,224]), transforms.ToTensor(),transforms.ConvertImageDtype(torch.uint8)])\n",
    "train_data = datasets.ImageFolder(train_dir, transform = data_transform)\n",
    "\n",
    "# get number of sample in each subset\n",
    "num_nv = dict(Counter(train_data.targets))[0]  # non_venonum\n",
    "num_v = dict(Counter(train_data.targets))[1]   # venoum\n",
    "\n",
    "nv_dset = torch.utils.data.Subset(train_data, range(0, num_nv))  # 0 - num_nv are the images of non venomous \n",
    "dataloader = torch.utils.data.DataLoader(nv_dset, batch_size=num_nv) \n",
    "images, labels = next(iter(dataloader))\n",
    "\n",
    "# path to save augmented pitures\n",
    "path = os.path.join(data_dir, 'train_cutout/Non Venomous')  # the file directory where to save the augmented image\n",
    "if (os.path.isdir(path)): None\n",
    "else: os.mkdir(path) \n",
    "\n",
    "    \n",
    "augmenter = transforms.RandAugment()  # use the randaugmentation from the pytorch library\n",
    "\n",
    "    \n",
    "def apply_mask(image, size=90, n_squares=1):  # randomly cutout the images\n",
    "    h, w, channels = image.shape\n",
    "    \n",
    "    new_image = image\n",
    "    for _ in range(n_squares):\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - size // 2, 0, h) # the location of the y1 \n",
    "        y2 = np.clip(y + size // 2, 0, h) # the location of the y2\n",
    "        x1 = np.clip(x - size // 2, 0, w) # the location of the x1\n",
    "        x2 = np.clip(x + size // 2, 0, w) # the location of the x2\n",
    "        new_image[y1:y2,x1:x2,:] = 0    # set the target area to 0 \n",
    "    return new_image\n",
    "\n",
    "num_aug = 3 # the number of time to do the augmentation \n",
    "\n",
    "for i in range(len(images)):\n",
    "\n",
    "    for j in range(num_aug):\n",
    "        img = augmenter(images[i])\n",
    "        newimage = np.transpose(img.numpy(), (1, 2, 0))\n",
    "        cutimage = apply_mask(newimage)\n",
    "    #     print(cutimages)\n",
    "    #     print(cutimages.shape)\n",
    "#         plt.imshow(cutimages)\n",
    "        plt.imsave(path +'/img' + str(i) + \"_\" + str(j) + '.jpg',cutimage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ac725",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aug = 3 # the number of time to do the augmentation \n",
    "\n",
    "for i in range(len(images)):\n",
    "    \n",
    "    img_ori = images[i]\n",
    "    for j in range(num_aug):\n",
    "        \n",
    "        print(1)\n",
    "        images = augmenter(img_ori)\n",
    "\n",
    "        newimage = np.transpose(images.numpy(), (1, 2, 0))\n",
    "        cutimages = apply_mask(newimage)\n",
    "    #     print(cutimages)\n",
    "    #     print(cutimages.shape)\n",
    "        plt.imshow(cutimages)\n",
    "        plt.imsave(path +'/img' + str(i) + '.jpg',cutimages)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81bb7d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2af20b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228, 3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de04dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
